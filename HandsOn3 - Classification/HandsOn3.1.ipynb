{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On 3.1 - Machine learning and classification\n",
    "\n",
    "In this laboratory you will learn about classification problems and how they can be approached using a\n",
    "category of tree-based models. In particular, you will use a decision tree from scikit-learn. You will see\n",
    "it in action with different datasets and understand its points of strength and weaknesses. Then, you will\n",
    "implement your own version of a random forest (already given), starting from scikit-learn’s decision trees.\n",
    "\n",
    "## 1. Wine classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "print(dataset.keys())\n",
    "X = dataset[\"data\"]\n",
    "y = dataset[\"target\"]\n",
    "feature_names = dataset[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['DESCR'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will use sklearn’s `DecisionTreeClassifier` to build a decision tree for the wine dataset. You can read more about this class on the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "\n",
    "1. Based on your $X$ and $y$, answer the following questions:\n",
    "\n",
    "- How many records are available?\n",
    "- Are there missing values?\n",
    "- How many elements does each class contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here ##\n",
    "num_records = \n",
    "num_missing = \n",
    "\n",
    "print(f\"Number available records: {num_records}\")\n",
    "print(f\"Number missing values: {num_missing}\")\n",
    "\n",
    "print(f\"Elements does each class\") # print a list or whatever you want\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a `DecisionTreeClassifier` object with the default configuration (i.e. without passing any\n",
    "parameters to the constructor). Train the classifier using your $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now that you have created a tree, you can visualize it. Sklearn offers two functions to visualize decision trees. The first one, `plot_tree()`, plots the tree in a matplotlib-based, interactive window.\n",
    "An alternative way is using `export_graphviz()`, which exports the tree as a DOT file. DOT\n",
    "is a language for describing graph (and, as a consequence, trees). From the DOT code, you can\n",
    "generate the resulting visual representation either using specific Python libraries. We recommend using the first approach, which is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Idea: can you increase the plot size? ax can be something you already used\n",
    "\n",
    "#####\n",
    "p = plot_tree(clf, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you successfully plotted a tree, you can take a closer look at the result and draw some conclusions. In particular, what information is contained in each node? Take a closer look at the leaf\n",
    "nodes. Based on what you know about overfitting, what can you learn from these nodes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Given the dataset $X$, you can get the predictions of the classifier (one for each entry in X) by calling the `predict()` of `DecisionTreeClassifier`. Then, use the `accuracy_score()` function (which you can import from `sklearn.metrics`) to compute the accuracy between two lists of values (`y_true`,\n",
    "the list of “correct” labels, and `y_pred`, the list of predictions made by the classifier). Since you\n",
    "already have both these lists ($y$ for the ground truth, and the result of the `predict()` method for the prediction), you can already compute the accuracy of your classifier. What result do you get? Does\n",
    "this result seem particularly **high/low**? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠️ Try to answer this question before going on"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now, we can split our dataset into a training set and a test set. We will use the training set to train a model, and to assess its performance with the test set. Sklearn offers the `train_test_split()`\n",
    "function to split any number of arrays (all having the same length on the first dimension) into two\n",
    "sets. You can refer to the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to understand how it can be used. You can use an\n",
    "80/20 train/test split. If used correctly, you will get 4 arrays: `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    "    Try to compute the balance between the classes for train and test sets, is it fine?\n",
    "> **Hint:** Consider attributes such as `shuffle` and `stratify` to answer this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train\")\n",
    "for c in [0,1,2]:\n",
    "    print(c, len(y_train[y_train==c])/len(y_train))\n",
    " \n",
    "print(\"\\ntest\")\n",
    "for c in [0,1,2]:\n",
    "    print(c, len(y_test[y_test==c])/len(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, train a new model using (`X_train`, `y_train`) and compute the accuracy with (`X_test`,\n",
    "`y_test`). How does this value compare to the previously computed one? Is this a more reasonable\n",
    "value? Why? \n",
    "\n",
    "    This should give you a good idea as to why training and testing on the same dataset returns meaningless results. Try also to use the `classification_report` function, which returns various metrics (including the accuracy) for each of the classes of the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "g = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, square=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "7. So far, you have only used decision trees using the default configuration. The “default” decision tree might not be the best option in terms of performance to fit our dataset. Let's try now to perform a “grid search”: you will define a set of possible configurations and, for each configuration, build a classifier. The idea is to test the performance of each classifier and identify that configuration that produces the best model.\n",
    "Based on the documentation and on our theoretical knowledge, we can find a list of all parameters you to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [None, 2, 3, 4, 5, 6],\n",
    "    \"min_impurity_decrease\": [0, .01, .03, .07, .09, .11]\n",
    "}\n",
    "\n",
    "accuracies = []\n",
    "grid = ParameterGrid(params)\n",
    "    \n",
    "###################\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The best model is made with {grid[np.argmax(accuracies)]}\")\n",
    "print(f\"getting accuracy of {max(accuracies)}\")\n",
    "\n",
    "g = sns.heatmap(pd.DataFrame(grid).assign(acc=accuracies).pivot(columns='max_depth', index='min_impurity_decrease', values='acc'), square=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. With the previous exercise, we have defined what the **best hyperparameters** configuration should be. We also have an estimate of how good the best performing configuration is (the one we obtained on the test set). The problem, is that this estimate is, in a way, biased: **we have selected the configuration that performs best on our test set**. Much like there is a risk of overfitting on the training data because we learn it \"too well\", there is also a risk of overfitting on the data we use for the validation, if we keep tweaking and adjusting our model to optimize the performance on that dataset.\n",
    "\n",
    "    For this reason, we typically use **three** separate datasets: the *training*, the *validation* and the *test* set. We use the first one for training the model, the second one to select which model to use (of the many we can create) and the last one to get a final estimate of how good our classifier is.\n",
    "\n",
    "    This is why, in cases where data is limited, we typically use the so-called **k-fold cross validation**. In short, we use the training set for both training and validation, by rotating the data we use for either operation.\n",
    "\n",
    "    We can now split our original dataset (X, y) into a test set (`X_test`, which we will set aside and use only for the final evaluation) and a set (`X_train_valid`) that we will use for both training and validation (through k-fold cross validation).\n",
    "\n",
    "    At each iteration a different fold is used for the validation, while the rest of X_train_valid will be used for the training. This means that, for each classifier, we will get k accuracies, which we then need to aggregate to obtain a single, **overall accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train_valid, X_test,\\\n",
    "y_train_valid, y_test = train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = list(ParameterGrid(params))[np.argmax(accuracies)]\n",
    "clf = DecisionTreeClassifier(**best_config)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. More challenging tasks (MNIST)\n",
    "\n",
    "We are now facing a problem for which our DecisionTree is not enough, we will then implement something stronger your own version of a random forest, using the trees available from `scikit-learn`. You will then train the random forest using the MNIST dataset and assess its performance\n",
    "compared to decision trees.\n",
    "\n",
    "1. Load the MNIST dataset into memory. We have a total of  70,000 digits images. You have to split it into a training set (60,000 digits) and a test set (10,000 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "dataset = fetch_openml(\"mnist_784\")\n",
    "X = dataset[\"data\"].values\n",
    "y = dataset[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(14, 4))\n",
    "for i, rnd in enumerate(np.random.randint(0, len(X), 3)):\n",
    "    ax[i].imshow(X[rnd].reshape(28, 28))\n",
    "    ax[i].set_title(f\"Label: {y[rnd]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a single decision tree (with the default parameters) on the training set, then compute its\n",
    "accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement a random forest. A random forest is an ensemble approach: it trains multiple trees on different portions of the dataset. This **lowers**\n",
    "the chance of overfitting on the dataset (the single tree might overfit its portion of data, but the overall “forest” will likely not).\n",
    "\n",
    "Each tree, bases each split decision using a subset of all features. The size of this subset, is often selected to be the square root of the total number of features available, but different. This parameter can be defined for each decision tree through the **`max_features`** parameter. When building a tree, a random sample of `max_features` features will be extracted and used to select the split. Another important parameter for random forests is the number of trees used (here `n_estimators`).\n",
    "\n",
    "During the prediction of a new list of points, each tree of the random forest will make its prediction. Then, through majority voting, the overall label assignment is made. _Majority voting is just a fancy way of saying that the class selected by the highest number of trees is selected_\n",
    "\n",
    "![](https://www.memecreator.org/static/images/memes/5153568.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode # performs the majority vote strategy\n",
    "\n",
    "class MyRandomForestClassifier:\n",
    "    def __init__(self, n_estimators=10, max_features='sqrt'):\n",
    "        self.trees = [ DecisionTreeClassifier(max_features=max_features)\\\n",
    "                       for _ in range(n_estimators) ]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for tree in self.trees:\n",
    "            # Here we are taking a subset of X, but with ALL the features\n",
    "            subset = np.random.choice(range(X.shape[0]),\\\n",
    "                                      size=X.shape[0],\\\n",
    "                                      replace=True)\n",
    "            tree.fit(X[subset], y[subset])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # get predictions of all trees for X \n",
    "        predictions = [ tree.predict(X).astype(int) for tree in self.trees ]\n",
    "        result = mode(predictions, axis=0, keepdims=True)[0][0]\n",
    "        return [str(v) for v in result] # label is a string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now train your random forest with the 60,000 points of the training set and compute its accuracy against the test set. How does the random forest behave? How does it compare to a decision tree? How does this performance vary as the number of estimators grow? Try values from 10 to 100 (with steps of 10) for ``n_estimators``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MyRandomForestClassifier(??)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many instruments do we have?\n",
    "\n",
    "Concerning classification, we can test many algorithms which may have better depending on the task. On the [Official documentation](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) of `sklearn` you can find a list of the most used, which will be useful for the next part of the HandsOn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2abf257b2adb4079ad80be3a8eaee4074fa006c61c8cda918f5cbe42d818212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
