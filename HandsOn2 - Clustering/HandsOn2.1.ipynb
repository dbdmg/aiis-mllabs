{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On 2 - Machine learning on clustering\n",
    "\n",
    "The purpose of this laboratory is to make you practice with the data preparation process. More specifically, you will tackle the task with tabular and textual input data, learning how to handle anomalies in the data, missing values and more.\n",
    "\n",
    "\n",
    "### 1.0 Plotting\n",
    "\n",
    "In order to complete some of the optional exercises you will need the `matplotlib` plotting library. You can read more about it on its [official documentation](https://matplotlib.org/). You can check if it is already installed by running `import matplotlib` in Jupyter.\n",
    "Among its numerous functionalities, you will use the histogram plotting function in this laboratories. An histogram is a simple representation of the distribution of numerical data. It presents the binned range of values on x-axis (i.e. a series of intervals in which data is divided) and the frequency of each bin on the y-axis. \n",
    "Here it is a short python script on how to use matplotlib to plot the histogram of values contained in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import gauss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "l = [gauss(0, 1) for _ in range(500)]\n",
    "\n",
    "plt.hist(l)\n",
    "plt.title('Gaussian distribution (mu=0, sigma=1)')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Global Land Temperature\n",
    "\n",
    "The Global Land Temperature (GLT) dataset is a large collection of measurements actively maintained by Berkeley Earth. It contains the raw source data measured with stations all around the globe, plus an intermediate format and several formatted output files. Data span from ∼1750 up to recent days with monthly and daily availability. Measurements are provided by hemispheres, states, countries, cities and more. You can read more about the dataset at the [Berkeley Earth website](https://berkeleyearth.org/data/). \n",
    "\n",
    "For the purpose of this laboratory you will work on a modified, smaller but dirtier, version of the original GLT dataset, to stress the importance of data preprocessing. More specifically, this didactic version contains the formatted output files of the major cities of the globe with monthly granularity. For the sake of simplicity, the analysis will range between almost two centuries (i.e. between the years 1817 and 2012).\n",
    "\n",
    "The dataset is composed of **∼200k rows** corresponding to the measurements taken the first day of the month in a given city. Each measurement is then described by 7 values:\n",
    "- Date, when the measurement was taken\n",
    "- AverageTemperature\n",
    "- AverageTemperatureUncertainty\n",
    "- City, from which the measurement was taken\n",
    "- Country\n",
    "- Latitude\n",
    "- Longitude\n",
    "The dataset is available in CSV format. You can find it at the following URL`\n",
    "\n",
    "https://raw.githubusercontent.com/dbdmg/data-science-lab/master/datasets/GLT_filtered.csv\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The main goal of this exercise is to learn how to clean a real-world dataset searching for anomalies, such as missing values or outliers, in its data.\n",
    "\n",
    "**Prerequisites.** There are many ways to handle missing values. One can decide to delete a row of the dataset based on whether a missing value is present or not. This strategy can be adopted when the dataset is large and the information loss does not affect the overall distribution. Another common solution is to fill every missing value. If data has not a specific order, they can be replaced with the mean (or the median) of the involved attribute. Temporal data, instead, allow to replace missing values with values of adjacent rows, e.g. by averaging them. Clearly, this technique is possible if the data type allows to compute the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Global Land Temperature dataset as a list of lists. Before starting, take a moment to better inspect the attributes you are going to work on. How many of them are nominal, how many continuous or discrete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/dbdmg/data-science-lab/master/datasets/GLT_filtered.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyze the attribute **AverageTemperature**, which contains missing values. Fill any gap with the arithmetic mean among the closest antecedent and the closest successive measurements in time, **taken in the same city**. Assume the following rules for edge cases:\n",
    "    \n",
    "    **(a)** it can happen that a missing value does not have a preceding (or successive) measurement. This happens when the missing value is the first (or last) value of the dataset. If this is the case, consider the missing value to be preceded (or followed) by a 0, then compute the mean accordingly.\n",
    "\n",
    "    ```\n",
    "    original_list = [ '', 5, 6, '' ]\n",
    "    step_1 = [ 2.5, 5, 6, '' ] # (0 + 5) / 2\n",
    "    step_2 = [ 2.5, 5, 6, 3 ] # (6 + 0) / 2`\n",
    "    ```\n",
    "\n",
    "    **(b)** if there are consecutive missing values, just compute them in temporal order and use the newly inserted values to evaluate the following ones. Here it is an example with a simple list where both (a) and (b) rules have been applied:`\n",
    "    \n",
    "    ```\n",
    "    original_list = [ '', '', 24, 28.9 ]\n",
    "    step_1 = [ 12, '', 24, 28.9 ] # (0 + 24) / 2\n",
    "    step_2 = [ 12, 18, 24, 28.9 ] # (12 + 24) / 2\n",
    "    ```\n",
    "> **Note:** Missed values in pandas Series are `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['AverageTemperature', 'AverageTemperatureUncertainty']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(data, cities):\n",
    "    \"\"\"\n",
    "    data: an array/pd.Series with the values to fill.\n",
    "    cities: an array/pd.Series with the cities associated with the values.\n",
    "    \"\"\"\n",
    "    filled_data = data.copy()\n",
    "    ##\n",
    "    \n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'AverageTemperature': [np.nan, 12, np.nan, np.nan, np.nan, 15, np.nan],\n",
    "    'City': ['Rome', 'Rome', 'Rome', 'Turin', 'Turin', 'Turin', 'Turin']})\n",
    "\n",
    "print('Original list:', test_df.AverageTemperature)\n",
    "test_df.val = fill_gaps(test_df.AverageTemperature, test_df.City)\n",
    "print('Filled list:', test_df.AverageTemperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result:\n",
    "df.AverageTemperature = fill_gaps(df.AverageTemperature, df.City)\n",
    "df.AverageTemperature.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a function that, given the name of a city and an integer N > 0, prints:\n",
    "    (a) the top N hottest measurements;\n",
    "    (b) the top N coldest measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hottest_coolest(city, N, data):\n",
    "    \"\"\"\n",
    "    data: the whole dataframe\n",
    "    ----\n",
    "    hint: you can also do as before, with 2 separate vectors for cites and values.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hottest_coolest('Rome', 3, df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let’s search for other anomalies in data distribution with the help of matplotlib. Plot the distribution of the average land temperatures for Rome and Bangkok using the aforementioned histogram\n",
    "plotting function.\n",
    "\n",
    "> **Info:** calling the plt.hist() method twice will draw the second histogram onto the canvas generated by the first call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Rome and Bangkok have very different temperature distributions, but this seems\n",
    "plausible. What it looks strange is the large difference in their temperatures’ magnitude. Is it possible that all sensors from Bangkok stations, along the entire time-span, were faulty? Could they were configured to use another representation of the temperature measurement? Can you figure out a data cleaning step to solve it?\n",
    "\n",
    "_Before continuing, try to answer to these questions._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. One might think that Bangkok sensor provide temperature samples in degrees Fahrenheit while\n",
    "the ones located in Rome use the Celsius notation, which is the common representation in the whole dataset. Write a function to transform Fahrenheit measurements back to Celsius, apply it to your data and plot the two distribution again.\n",
    "\n",
    "> **Info:** remember that the mapping function from Celsius to Fahrenheit is the following\n",
    "> $$T_F = 1.8 · T_C + 32$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rome average temperature: {df[df.City=='Rome'].AverageTemperature.mean():.2f}, Standard deviation: {df[df.City=='Rome'].AverageTemperature.std():2f}\\n\")\n",
    "print(f\"Bangkok average temperature: {df[df.City=='Bangkok'].AverageTemperature.mean():.2f}, Standard deviation: {df[df.City=='Bangkok'].AverageTemperature.std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2abf257b2adb4079ad80be3a8eaee4074fa006c61c8cda918f5cbe42d818212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
