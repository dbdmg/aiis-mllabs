{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On 1 - Introduction to Pyspark\n",
    "\n",
    "The objective of this laboratory is to start playing around with Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Warm up\n",
    "\n",
    "In this exercise you will run a simple Spark program in a Jupyter notebook.\n",
    "Create a **SparkContext**, then execute the following lines of code which sum the integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'sc' not exists: create\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master('local[1]').appName('Lab1').getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/inputdata1.txt\")\n",
    "print(f\"The elements of the RDD are in the form:\\n{rdd.take(3)}\\n\")\n",
    "fieldsRdd = rdd.map(lambda line: line.split(\",\"))\n",
    "valueRdd = fieldsRdd.map(lambda l: int(l[1]))\n",
    "valueSum = valueRdd.reduce(lambda v1, v2: v1+v2)\n",
    "print(\"The sum is:\", valueSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, modify the previous code to do the following:\n",
    "\n",
    "- Filter the values greater than  5\n",
    "- Compute the product of those lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "filteredRdd = valueRdd.filter(lambda v: v>5)\n",
    "filteredProd = filteredRdd.reduce(lambda v1, v2: v1*v2)\n",
    "print(f\"The product is: {filteredProd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many distinct characters are used in the names in the input dataset. \n",
    "\n",
    "**Remember** that to do this task, you need to take the name from each row of the rdd, and than make it _flat_ to get one rdd of single letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "charactersRdd = fieldsRdd.flatMap(lambda l: l[0]).distinct()\n",
    "print(\"The number of distinct characters is:\", charactersRdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word count\n",
    "\n",
    "In this first part, you want to create a word count application and apply it to a sample text. \n",
    "This means that you have to read the file, split it into words and get how many times each word occurs.\n",
    "\n",
    "You finally have to store the result in the format `word\\tfreq`.\n",
    "\n",
    "**Remember:** differently from the previous task, you now need to perform a count _for each word_. How to do that?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex can be used to remove punctuation ('word' and 'word,' are the same)\n",
    "\n",
    "danteRdd = sc.textFile(\"data/commedia.txt\")\n",
    "\n",
    "### WRITE YOUR CODE HERE ###\n",
    "# words_rdd = dante_rrd.flatMap(lambda l: l.split())\n",
    "wordsRdd = danteRdd.flatMap(lambda l: re.sub(r\"[^\\w\\s]\", \"\", l).split())\n",
    "\n",
    "wordsPairRdd = wordsRdd.map(lambda w: (w, 1))\n",
    "countsPairRdd = wordsPairRdd.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "countsPairRdd.map(lambda v: f\"{v[0]}\\t{v[1]}\").saveAsTextFile(\"out1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably end up with a folder (`./out1`) with more files inside. \n",
    "- Why the result is split into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You assume to start with one file with the word frequencies in\n",
    "the Amazon food reviews dataset, in the format `word\\tfreq`, where freq is an integer.\n",
    "Your task is to write a Spark application to filter these results, analyze the filtered data and\n",
    "compute some statistics on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics on frequences\n",
    "\n",
    "You now have a large file with the word frequencies, in the format `word\\tfreq`, where freq is an integer and \\t means the tabulation character. The result would be the same if you start with `counts_pairRdd`, but please start reading a file again to get practice! If you have problems on the first part, you can start with the sample files in `data/out1`.\n",
    "\n",
    "Read the data and parse the content. This means separate the `word` and `freq` and convert `freq` to an integer.\n",
    "\n",
    "Next, print the $3$ most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output folders\n",
    "# I also define a variable containing the prefix I am interested in\n",
    "inputPath  = \"out1/\"\n",
    "outputPath = \"out2/\" \n",
    "prefix = \"me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data\n",
    "wordsFrequenciesRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the lines containing words that start with the prefix “me”\n",
    "### WRITE YOUR CODE HERE ###\n",
    "selectedLinesRDD = wordsFrequenciesRDD.filter(lambda line: line.startswith(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "# Print on the standard output the following statistics\n",
    "# - The number of selected lines\n",
    "\n",
    "numLines = selectedLinesRDD.count()\n",
    "print(f\"Num. selected lines: {str(numLines)}\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "# Print on the standard output the following statistics\n",
    "# - The maximum frequency (maxfreq) among the ones of the selected lines (i.e., the maximum value of freq in the lines obtained by applying the filter).\n",
    "\n",
    "# Select the values of frequency\n",
    "maxfreqRDD = selectedLinesRDD.map(lambda line: float(line.split(\"\\t\")[1]))\n",
    "\n",
    "# Compute the maximu value\n",
    "maxfreq = maxfreqRDD.reduce(lambda freq1, freq2: max(freq1, freq2) )\n",
    "\n",
    "# Print maxfreq on the standard output\n",
    "print(\"Maximum frequency: \"+ str(maxfreq) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extra filters\n",
    "\n",
    "Extend the previous application. Specifically, in the second part of your application, among the lines selected by the first filter, you have to apply another filter to select only the most frequent words. Specifically, your application must select those lines that contain words with a frequency (_freq_) greater than 80% of the maximum frequency (_maxfreq_) computed before.\n",
    "\n",
    "Hence, implement the following filter:\n",
    "- Keep only the lines with a frequency freq greater than 0.8*_maxfreq_.\n",
    "\n",
    "Finally, perform the following operations on the selected lines (the ones selected by applying both filters):\n",
    "- Count the number of selected lines and print this number on the standard output\n",
    "- Save the selected words (without frequency) in an output folder (one word per line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "# Keep only the lines with a frequency freq greater than 0.8*maxfreq.\n",
    "selectedLinesMaxFreqRDD = selectedLinesRDD.filter(lambda line: float(line.split(\"\\t\")[1])>0.8*maxfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "# Count the number of selected lines and print this number on the standard output\n",
    "numLinesMaxfreq = selectedLinesMaxFreqRDD.count()\n",
    "print(\"Num. selected lines with freq > 0.8*maxfreq: \"+ str(numLinesMaxfreq) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the words (first field)\n",
    "selectedWordsRDD = selectedLinesMaxFreqRDD.map(lambda line: line.split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected words (without frequency) in an output folder (one word per line)\n",
    "selectedWordsRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
